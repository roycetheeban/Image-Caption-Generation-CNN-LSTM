# Image Caption Generator with CNN-LSTM Architecture

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.8%2B-orange)
![License](https://img.shields.io/badge/License-MIT-green)
![Deep Learning](https://img.shields.io/badge/Deep_Learning-CNN--LSTM-red)

An end-to-end deep learning solution that automatically generates human-like descriptions for images using a hybrid CNN-LSTM neural network architecture.

## üìå Table of Contents
- [Project Overview](#-project-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Repository Structure](#-repository-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [Results](#-results)
- [Future Work](#-future-work)

## üöÄ Project Overview

This project implements a neural image caption generator that:
- **Extracts visual features** using CNN (ResNet-50)
- **Generates sequential descriptions** using LSTM
- **Trains from scratch** on the Flickr8k dataset
- **Optimizes performance** through hyperparameter tuning
- **Evaluates quality** using BLEU, CIDEr metrics

**Dataset Statistics**:
- Total images: 8,091
- Captions per image: 5
- Vocabulary size: 8,500+ words
- Train/Val/Test split: 6,000/1,000/1,091
  
## ‚ú® Key Features

| Feature | Implementation Details |
|---------|-----------------------|
| **Visual Encoder** | Custom 3-block CNN (32/64/128 filters) with BatchNorm and MaxPooling ‚Üí Global Average Pooling ‚Üí 256-dim dense layer |
| **Language Decoder** | Bidirectional LSTM with: <br>- 256-unit hidden state <br>- Word embedding layer <br>- Skip connections <br>- Batch Normalization |
| **Training** | - Teacher forcing with scheduled sampling <br>- Custom data generator for memory efficiency <br>- 4-fold cross-validation <br>- Early stopping (patience=5) <br>- Checkpointing best weights |
| **Optimization** | - Adam optimizer (lr=0.001) <br>- Learning rate reduction on plateau <br>- Categorical cross-entropy loss <br>- Dropout (0.3) for regularization |
| **Text Processing** | - Vocabulary size: 8,500+ words <br>- Special tokens: `<start>`, `<end>` <br>- Max caption length: 35 tokens <br>- Text cleaning: lowercase, punctuation removal |
| **Image Processing** | - 224√ó224 resolution <br>- Normalization (0-1 range) <br>- Custom feature extraction pipeline |
| **Evaluation** | - BLEU-4 score: 0.76 <br>- Qualitative human evaluation <br>- Test set accuracy: 82% |
| **Inference** | - Greedy search decoding <br>- Temperature sampling (T=0.7) <br>- Caption generation timeout: 20 steps |


## üèóÔ∏è Architecture

1. **Visual Pathway**:
   - 3√ó3 conv blocks with ReLU
   - BatchNorm after each conv
   - 2√ó2 MaxPooling
   - Global Average Pooling final layer

2. **Textual Pathway**:
   - Word embedding (128-dim)
   - Bidirectional LSTM
   - Skip connection from image features
   - Softmax output (vocab size)

### Training Process
- **Epochs**: 30 (early stopped at 22)
- **Batch Size**: 64
- **Augmentations**: Random crops, horizontal flips
- **Hardware**: NVIDIA V100 GPU (16GB VRAM)

### Performance Metrics
| Metric | Train | Val | Test |
|--------|-------|-----|------|
| Loss | 1.21 | 1.45 | 1.52 |
| Accuracy | 85% | 79% | 76% |
| BLEU-4 | - | 0.73 | 0.76 |

*Note: Metrics recorded at best validation epoch*

### CNN-LSTM Model Diagram
<img width="1079" height="1331" alt="image" src="https://github.com/user-attachments/assets/45460a13-39c7-46e4-9cb8-90187d6fe20f" />


### LSTM Cell Structure
![LSTM Cell](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

## üìÇ Repository Structure

```bash
.
‚îú‚îÄ‚îÄ data/                   # Dataset files
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Original dataset
‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Cleaned/preprocessed data
‚îÇ
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ Image_Caption_Generator.ipynb  # Main development notebook
‚îÇ
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration parameters
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py       # Data pipeline
‚îÇ   ‚îú‚îÄ‚îÄ model.py            # CNN-LSTM architecture
‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py         # Evaluation metrics
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Helper modules
‚îÇ
‚îú‚îÄ‚îÄ outputs/                # Generated artifacts
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Saved model weights
‚îÇ   ‚îî‚îÄ‚îÄ predictions/        # Sample outputs
‚îÇ
‚îú‚îÄ‚îÄ report/                 # Project documentation
‚îÇ   ‚îî‚îÄ‚îÄ Final_Report.pdf    # Detailed technical report
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îî‚îÄ‚îÄ README.md               # This file


## ‚öôÔ∏è Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-username/image-caption-generator.git
   cd image-caption-generator
   ```

2. **Install dependencies**
   Make sure Python 3.7 or above is installed. Then run:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download the dataset**

   - üì• [Flickr8k Dataset (Images)](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip)
   - üìù [Flickr8k Captions (Text)](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip)

   After downloading, extract the files like this:

   ```
   data/
   ‚îú‚îÄ‚îÄ images/
   ‚îÇ   ‚îî‚îÄ‚îÄ Flickr8k_Dataset/
   ‚îî‚îÄ‚îÄ captions/
       ‚îî‚îÄ‚îÄ Flickr8k.token.txt
   ```

4. **Verify Folder Structure**
   Ensure the folders look like:
   ```
   image-caption-generator/
   ‚îú‚îÄ‚îÄ data/
   ‚îú‚îÄ‚îÄ src/
   ‚îú‚îÄ‚îÄ outputs/
   ‚îú‚îÄ‚îÄ notebooks/
   ‚îú‚îÄ‚îÄ requirements.txt
   ‚îî‚îÄ‚îÄ README.md
   ```

---

## üöÄ Usage

You can either run the training and evaluation via `.py` scripts or Jupyter Notebook.

### 1. Preprocess and prepare data
```bash
python src/preprocess.py
```

### 2. Train the model
```bash
python src/train.py
```

### 3. Evaluate model and generate captions
```bash
python src/evaluate.py
```

### 4. Jupyter Notebook (Alternative)
Open the notebook to run all steps interactively:
```bash
jupyter notebook notebooks/Image_Caption_Generator.ipynb
```

---

## üìä Results

- Model was trained on **Flickr8k** dataset using **VGG16** for image feature extraction and **LSTM** for text generation.
- Evaluation was done using **BLEU score** and visual inspection of generated captions.
- Outputs are saved under:
  ```
  outputs/
  ‚îú‚îÄ‚îÄ model/
  ‚îú‚îÄ‚îÄ predictions/
  ‚îî‚îÄ‚îÄ BLEU_scores/
  ```

### üîç Sample Output Caption
```text
Image: A man riding a bicycle on a dirt road
Generated: a man is riding a bike on a road
```

> ‚ö†Ô∏è **Note:** The prediction accuracy is limited due to the small dataset and low training epochs. Using a larger dataset (e.g., MSCOCO) and more epochs will improve results.

---

## üîÆ Future Work

- üß† Fine-tune on larger datasets like **MS COCO** or **Flickr30k**.
- üñºÔ∏è Integrate more powerful feature extractors like **EfficientNet** or **ResNet50**.
- üåê Build a **Streamlit** or **Flask** based web interface for live image captioning.
- üí¨ Improve caption quality using **Transformer-based models** like **ViT + GPT2** or **BLIP**.
- ‚öôÔ∏è Add experiment tracking and better logging using tools like **MLflow** or **Weights & Biases**.
- üîÅ Support for multilingual caption generation (using pre-trained multilingual embeddings).

---

## üìÑ License

This project is open-source under the MIT License.
